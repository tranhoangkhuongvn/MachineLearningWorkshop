{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Objectives\n",
    "- Understand how to apply machine learning model to classification tasks\n",
    "- Explain the working mechanism of LogisticRegression: probability output, log-likelyhood etc...\n",
    "- Explain performance metric for classification tasks\n",
    "- Dataset available: Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eventhough there is the word `regression` in the name of the algorithm, this is a classification learning algorithm. The `regression` here stems from the fact that the formula of the algorithm is similar to that of linear regression.\n",
    "- If the output label only has 2 different classes, then it is considered binary classification. If there are more than two different classes, it is multi-class classification\n",
    "- Let's start with a simple example of binary classification (2 classes) using a toy dataset to classify student's admission to university based on 2 exam scores\n",
    "<center><img src='./assets/linear_classifier.png' width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the difference between linear regression and logistic regression? In linear regression problems, the target variable $y$ is a continuous value (e.g $y \\in \\mathbb{R}$). While in classification tasks, the output labels are in a discrete set of values. (e.g $y \\in  \\{-1,1\\}$ in binary classification, or $y \\in \\{1,2, \\ldots, k\\}$ for multi-class classification with $k$ different classes). \n",
    "In logistic classification, the output label is also a function of the linear combination of input features. However, it needs to be scaled and interpreted as a probability of belonging to certain classes. Let us denote $p_n = P(y = 1)$ as the probability of the output $y$ equals to a class (e.g classify an email as spam/ham). Let's reuse the simple 1-D example from Linear Regression with some modifications to model the Logistic Regression function.\n",
    "\n",
    "$$p_n = \\sigma(\\beta_0 + \\beta_1x)$$\n",
    "\n",
    "$\\sigma(.)$ is called a sigmoid function or a logistic function which serves the task of scaling the linear combination of input features to be between 0 and 1 which matches the range of a probability value. The predicted class $\\hat{y}$ is 1 if $p_n >= 0.5$ and 0 if $p_n < 0.5$.\n",
    "The value of the logistic function is as the below graph. If $t > 0$ then the output of $\\sigma(t)$ is positive, and it is negative if $t < 0$. It outputs $1/2$ if $t = 0$. In other words, if the linear combination of input features is positive, then the predicted output is 1, otherwise the predicted output is 0. (This is related to the vector of the hyperplane that classifies the data. Interested readers can look at the references materials).\n",
    "\n",
    "<center><img src='./assets/sigmoid_function.png' width=\"500\"></center>\n",
    "\n",
    "In summary:\n",
    "\n",
    "- Logistic regression outputs the probabilities of a specific class.\n",
    "- Those probabilities can be converted into class predictions.\n",
    "\n",
    "The logistic function has some nice properties:\n",
    "\n",
    "- Takes on an \"s\" shape\n",
    "- Output is bounded by 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Logistic Regression model to classify whether a student's performance on two exams can get him/her accepted to a university. The input features are the scores of exam 1 and 2. The output label will be either accepted `(1)` or not accepted `(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_decision_boundary, plot_2d_boundary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by loading the data\n",
    "data = pd.read_csv(\"data/logistic/data1.txt\", header = None, \n",
    "                   names = [\"Exam 1 Score\", \"Exam 2 Score\", \"Accepted\"])\n",
    "\n",
    "# initialize some useful variables\n",
    "m = len(data[\"Accepted\"])\n",
    "#x0 = np.ones(m)\n",
    "exam1 = np.array((data[\"Exam 1 Score\"]))\n",
    "exam2 = np.array((data[\"Exam 2 Score\"]))\n",
    "#X = np.array([x0, exam1, exam2]).T\n",
    "X = np.array([exam1, exam2]).T\n",
    "y = np.array(data[\"Accepted\"]).reshape((m,1))\n",
    "m, n = X.shape\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the score and the acceptance of the first few students:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of positive and negative examples\n",
    "pos = np.where(y==1)[0]\n",
    "neg = np.where(y==0)[0]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "# Plot examples\n",
    "plt.plot(X[pos, 0], X[pos, 1], 'b+', label='Admitted')\n",
    "plt.plot(X[neg, 0], X[neg, 1], 'yo', label='Not admitted')\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like it is linearly separable. Let's use Logistic Regression to find this line that classify the scores into \n",
    "Accepted/Not Accepted regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model and store the class predictions.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LogisticRegression model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process y to suit the input to train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to train and test sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and run the prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to Linear Regression, we can look at the model's coefficients through the model attributes: `logreg.coef_, logreg.intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function plot_2d_boundary to visualise the boundary with the corresponding coefficients\n",
    "# Try it on test set, training set or the entire dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough there might be some points misclassified, the line is still doing a pretty good job in keeping the majority of the points in the correct side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise ####\n",
    "- Spend some time looking at the options available in LogisticRegression page of scikit-learn documentation.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the LogisticRegression model into classifying survivorship of titanic passengers\n",
    "- Apply the Logistic Regression on a Kaggle dataset called Titanic survivorship\n",
    "- Given a set of passenger's attributes, we need to predict whether that passenger can stay alive from the disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set(font_scale=1.5);\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# Load the titanic dataset and check out its dimension, content, data type in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns description of the titanic dataset:\n",
    "<center><img src='./assets/titanic_columns.png' width=\"800\"></center>\n",
    "\n",
    "(Adapted from Kaggle Titanic dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data in the training dataset: Age and Cabin contain quite alot of missing data, 20% and 77% respectively\n",
    "titanic_train.isnull().sum() / titanic_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because there are approximately 20% of missing data for Age, we can try to figure out which values should we use to impute.\n",
    "- On the other hand, there are upto 77% missing data for Cabin, there is no point to impute this column since it is hard to figure which value should we use.\n",
    "- There are approximately 0.22% missing data for Embarked, we can just impute this column with the most occured value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Age variable:\n",
    "ax = titanic_train[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "titanic_train[\"Age\"].plot(kind='density', color='teal')\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select the median age to fill in\n",
    "titanic_train['Age'].mean(), titanic_train['Age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Boarded passengers grouped by port of embarking (C = Cherbourg, Q = Queenstown, S = Southampton):')\n",
    "print(titanic_train['Embarked'].value_counts())\n",
    "sns.countplot(x='Embarked', data=titanic_train, palette='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = titanic_train.copy()\n",
    "train_data[\"Age\"].fillna(titanic_train[\"Age\"].median(skipna=True), inplace=True)\n",
    "train_data[\"Embarked\"].fillna(titanic_train['Embarked'].value_counts().idxmax(), inplace=True)\n",
    "train_data.drop('Cabin', axis=1, inplace=True)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check to make sure there is no more missing data\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise adjusted age after modification, it looks closer to the bell curve of Gaussian distribution\n",
    "plt.figure(figsize=(15,8))\n",
    "ax = titanic_train[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "titanic_train[\"Age\"].plot(kind='density', color='teal')\n",
    "ax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\n",
    "train_data[\"Age\"].plot(kind='density', color='orange')\n",
    "ax.legend(['Raw Age', 'Adjusted Age'])\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two features SibSp and Parch seems to relate to each other from the column description. One thing we can do is to create a new column representing the fact that the traveler is going alone or with company. Then we can drop these 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['TravelAlone']=np.where((train_data[\"SibSp\"]+train_data[\"Parch\"])>0, 0, 1)\n",
    "train_data.drop('SibSp', axis=1, inplace=True)\n",
    "train_data.drop('Parch', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "\n",
    "Categorical data are represented using integers and hence some ordinal relationships may be wrongly interpreted by the learning algorithms.\n",
    "For example if there are five categories 'A', 'B', 'C', 'D' and 'E' that do not have any ordinal meanings, interprete them using integers from 1 to 5 may accidentially introduce comparability. Therefore, one-hot encoding can represent each of these categories using 5 binary vectors,or to be useful to the ML model, 4 of them are going to be used as the 5th vector can be infered from the first four. This is to remove collinearity between input features. Pandas has an option to remove this redundant column.\n",
    "\n",
    "For example, the season column which contains 4 integers: 1 - Spring, 2 - Summer, 3 - Autumn and 4- Winter can be represented as 4 binary vectors:\n",
    "- 1: [1, 0, 0, 0]\n",
    "- 2: [0, 1, 0, 0]\n",
    "- 3: [0, 0, 1, 0]\n",
    "- 4: [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try perform pd.get_dummies on some categorical column to see how it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For other categorical columns, we are going to convert them into one-hot encoding vector\n",
    "training=pd.get_dummies(train_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"], drop_first=True)\n",
    "#training.drop('Sex_female', axis=1, inplace=True)\n",
    "training.drop('PassengerId', axis=1, inplace=True)\n",
    "training.drop('Name', axis=1, inplace=True)\n",
    "training.drop('Ticket', axis=1, inplace=True)\n",
    "\n",
    "final_train = training\n",
    "final_train.to_csv('data/final_titanic_train.csv', index=False)\n",
    "final_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, whatever changes we made to the train data, make sure we also do the same to the test data. \n",
    "\n",
    "\n",
    "(*) Important to note here that we use the median values from the train data to impute test data to avoid data leaking issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = titanic_test.copy()\n",
    "test_data[\"Age\"].fillna(titanic_train[\"Age\"].median(skipna=True), inplace=True)\n",
    "test_data[\"Fare\"].fillna(titanic_train[\"Fare\"].median(skipna=True), inplace=True) #Fare has 1 missing value in the test data\n",
    "test_data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "test_data['TravelAlone']=np.where((test_data[\"SibSp\"]+test_data[\"Parch\"])>0, 0, 1)\n",
    "\n",
    "test_data.drop('SibSp', axis=1, inplace=True)\n",
    "test_data.drop('Parch', axis=1, inplace=True)\n",
    "\n",
    "testing = pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"], drop_first=True)\n",
    "#testing.drop('Sex_female', axis=1, inplace=True)\n",
    "testing.drop('PassengerId', axis=1, inplace=True)\n",
    "testing.drop('Name', axis=1, inplace=True)\n",
    "testing.drop('Ticket', axis=1, inplace=True)\n",
    "\n",
    "final_test = testing\n",
    "final_test.to_csv('data/final_titanic_test.csv', index=False)\n",
    "\n",
    "final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X_titanic = final_train.loc[:, ~final_train.columns.isin(['Survived'])]\n",
    "y_titanic = final_train['Survived']\n",
    "\n",
    "# use train/test split with different random_state values\n",
    "# we can change the random_state values that changes the accuracy scores\n",
    "# the scores change a lot, this is why testing scores is a high-variance estimate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, test_size=0.2, random_state=2)\n",
    "\n",
    "# check classification scores of logistic regression\n",
    "# TODO: create a logistic regression model called `logreg`\n",
    "# Your code goes here:\n",
    "\n",
    "\n",
    "print('Train/Test split results:')\n",
    "print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy tells the percentage of correctly identified label. The higher the accuracy the better. \n",
    "Highest accuracy score on Kaggle competition is approximately 90%, which requires a lot of feature engineering, feature selection, gridsearch as well as ensembling learning. For our current baseline, we have 78.2% accuracy with basic engineering effort and modeling. We are going to see how we can improve the performance in subsequent parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advanced-classification-metrics\"></a>\n",
    "## Classification Metrics\n",
    "\n",
    "---\n",
    "\n",
    "When we evaluate the performance of a logistic regression (or any classifier model), the standard metric to use is accuracy: How many class labels did we guess correctly? However, accuracy is only one of several metrics we could use when evaluating a classification model.\n",
    "\n",
    "$$Accuracy = \\frac{total~predicted~correct}{total~predicted}$$\n",
    "\n",
    "Accuracy alone doesn’t always give us a full picture.\n",
    "\n",
    "If we know a model is 75% accurate, it doesn’t provide any insight into why the 25% was wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a binary classification problem where we have 165 observations/rows of people who are either smokers or nonsmokers.\n",
    "\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "    <td style=\"text-align: center\"></td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **True positives (TP):** These are cases in which we predicted yes (smokers), and they actually are smokers.\n",
    "- **True negatives (TN):** We predicted no, and they are nonsmokers.\n",
    "- **False positives (FP):** We predicted yes, but they were not actually smokers. (This is also known as a \"Type I error.\")\n",
    "- **False negatives (FN):** We predicted no, but they are smokers. (This is also known as a \"Type II error.\")\n",
    "<table style=\"border: none\">\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none; vertical-align: bottom\">n = 165</td>\n",
    "    <td style=\"\"><b>Predicted: No</b></td>\n",
    "    <td style=\"\"><b>Predicted: Yes</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: No</b></td>\n",
    "    <td style=\"text-align: center\">TN = 50</td>\n",
    "    <td style=\"text-align: center\">FP = 10</td>\n",
    "    <td style=\"text-align: center\">60</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td><b>Actual: Yes</b></td>\n",
    "    <td style=\"text-align: center\">FN = 5</td>\n",
    "    <td style=\"text-align: center\">TP = 100</td>\n",
    "    <td style=\"text-align: center\">105</td>\n",
    "</tr>\n",
    "<tr style=\"border: none\">\n",
    "    <td style=\"border: none\"></td>\n",
    "    <td style=\"text-align: center\">55</td>\n",
    "    <td style=\"text-align: center\">110</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other classification losses that we cannot cover in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "TP = conf_matrix[1,1]\n",
    "TN = conf_matrix[0,0]\n",
    "FP = conf_matrix[0,1]\n",
    "FN = conf_matrix[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classification accuracy:\n",
    "print((TP + TN) / float(TP + TN + FP + FN)) # calculated using formula\n",
    "print(accuracy_score(y_test, y_pred)) # calculated using accuracy_score from metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classification error:\n",
    "print((FP + FN) / float(TP + TN + FP + FN))\n",
    "print(1 - accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sensitivity - Recall: when the actual value is Positive/ Yes, how often does the model predict correctly?\n",
    "print(TP / float(TP + FN))\n",
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Precision: when the model predicts positive class, how often does it predict correctly?\n",
    "print(TP / float(TP + FP))\n",
    "print(precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Discuss which metrics should we focus on for the following application?\n",
    "- Spam filter\n",
    "- Fraudulent transaction detector\n",
    "- Filter safe videos for kids\n",
    "- Detect shoplifters using surveillance system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we cannot have the case when we can have high precision (low FP) and high recall (low FN) because of precision/recall tradeoff. This involves setting a threshold from which positive class and negative class are separated. As you increase or decrease the threshold limit, it will increase/ decrease the FN and FP rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
