{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "- Used for both classification and regression\n",
    "- Aggregate the predictions of many individual predictors to achieve a better prediction accuracy. This is called the `wisdom of the crowd`\n",
    "- Can combine many types of predictors from different algorithms\n",
    "- Random Forest is one well-known algorithms where multiple Decision Tree predictors are used\n",
    "- Hard voting: the class with the majority vote is selected as the prediction\n",
    "- Soft voting: the class with the highest probability from individual predictors is used\n",
    "<center><img src='./assets/ensemble_learning.png' width=\"600\" height=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out a simple Ensemble Model built up by multiple classification algorithms. The toy dataset is available with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary, plot_2d_boundary, plot_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate the toy moon dataset \n",
    "X_moon, y_moon = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_moon_train, X_moon_test, y_moon_train, y_moon_test = train_test_split(X_moon, y_moon, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X_moon, y_moon, [-1.5, 2.5, -1, 1.5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create different types of classifiers and use VotingClassifier to combine them for classification task\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "tree_clf = DecisionTreeClassifier(max_depth= 3, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('tf', tree_clf), ('svc', svm_clf)],\n",
    "    voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the voting classifier\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Print out the accuracy for each classifier\n",
    "\n",
    "for clf in (log_clf, tree_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_moon_train, y_moon_train)\n",
    "    y_moon_pred = clf.predict(X_moon_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_moon_test, y_moon_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VotingClassifier slightly outperforms the remaining classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to do ensemble learning is to train different classifiers using different algorithms. However we can have another approach that is to train using the same training algorithms but on different subsets of the training data. If we sample the subset with replacement, we have `bagging`. If we sample the subset of training data without replacement, we have `pasting` method. The key advantages of ensemble learning using `bagging` or `pasting` are that only only they have higher accuracy but they also can be computed in parallel in different processing units or on different servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train an ensemble of `DecisionTreeClassifier` using `bagging` or `pasting` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(), n_estimators=500,\n",
    "        max_samples = 100, # each classifier trains on a subset of 100 training examples\n",
    "        bootstrap=True,  # True for `bagging` and False for `pasting`\n",
    "        n_jobs=-1, # use all CPU cores on your machine\n",
    "        oob_score=True #automatically use instances that were not sampled for evaluation, give better estimation for test data\n",
    ")\n",
    "\n",
    "\n",
    "bagging_clf.fit(X_moon_train, y_moon_train)\n",
    "\n",
    "# print this if using oob_score\n",
    "#print('Accuracy of bagging classifier:', bagging_clf.oob_score_)\n",
    "y_moon_pred = bagging_clf.predict(X_moon_test)\n",
    "print(accuracy_score(y_moon_test, y_moon_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary \n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done is actually called RandomForest algorithm, which is simply the ensemble of DecisionTreeClassifier. Here is another way to do it in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a RandomForestClassifier and fit it with the moon dataset\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise ####\n",
    "- Try to play around with the number of estimators and observe the decision boundary\n",
    "- RandomForest actually also measures the feature's importance when perform training. As a  result, we can look at this and see which feature was useful for the decision tree to classify the target. We can access these scores via the `feature_importances_ attribute of the `random_forest_clf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply an Ensemble Model on the Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "final_train = pd.read_csv('data/final_titanic_train.csv')\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X_titanic = final_train.loc[:, ~final_train.columns.isin(['Survived'])]\n",
    "y_titanic = final_train['Survived']\n",
    "\n",
    "# use train/test split with different random_state values\n",
    "# we can change the random_state values that changes the accuracy scores\n",
    "# the scores change a lot, this is why testing scores is a high-variance estimate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VotingClassifier to fit\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the score of VotingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation score to validate the accuracy of the training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random forest on our Titanic data and then look at its feature importance scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
